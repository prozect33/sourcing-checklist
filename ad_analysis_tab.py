from __future__ import annotations

import hashlib
import uuid
from typing import Dict

import numpy as np
import pandas as pd
import streamlit as st
import altair as alt

# ====== ì›ì‹œ ë°ì´í„° ì»¬ëŸ¼ëª…(í‘œì¤€) ======
DATE_COL = "ë‚ ì§œ"
KW_COL = "í‚¤ì›Œë“œ"
SURF_COL = "ê´‘ê³  ë…¸ì¶œ ì§€ë©´"
SURF_SEARCH_VALUE = "ê²€ìƒ‰ ì˜ì—­"
IMP_COL = "ë…¸ì¶œìˆ˜"
CLK_COL = "í´ë¦­ìˆ˜"
COST_COL = "ê´‘ê³ ë¹„"
ORD_COL = "ì´ ì£¼ë¬¸ìˆ˜(14ì¼)"
REV_COL = "ì´ ì „í™˜ë§¤ì¶œì•¡(14ì¼)"

# ====== ìœ í‹¸ ======
def _to_int(s: pd.Series) -> pd.Series:
    return pd.to_numeric(s, errors="coerce").fillna(0).round(0).astype(int)

def _to_float1(x: float | int) -> float:
    return float(np.round(float(x), 1))

def _to_date(s: pd.Series) -> pd.Series:
    txt = s.astype(str).str.strip()
    dt_general = pd.to_datetime(txt, errors="coerce")
    digits = txt.str.replace(r"[^0-9]", "", regex=True)
    dt_8 = pd.to_datetime(digits.where(digits.str.len() == 8), format="%Y%m%d", errors="coerce")
    dt_6 = pd.to_datetime(digits.where(digits.str.len() == 6), format="%y%m%d", errors="coerce")
    num = pd.to_numeric(txt, errors="coerce")
    num = num.where(num.between(20000, 60000))
    dt_serial = pd.to_datetime(num, unit="D", origin="1899-12-30", errors="coerce")
    dt = dt_general.fillna(dt_8).fillna(dt_6).fillna(dt_serial)
    return dt.dt.date

def _safe_div(a, b, default=0.0):
    a, b = float(a), float(b)
    return default if b == 0 else a / b

def _median_1d(s: pd.Series) -> float:
    return float(np.round(s.median(), 1)) if not s.empty else 0.0

def _median_int(v) -> int:
    s = pd.Series(v)
    return int(s.median()) if not s.empty else 0

def build_min_conversion_condition(df: pd.DataFrame) -> Dict[str, float]:
    return calc_base_threshold_t(df)

def format_min_condition_label(min_cond: Dict[str, float]) -> str:
    return (
        f"ìµœì†Œ ì „í™˜ ì¡°ê±´ (ìš´ì˜{int(min_cond['active_days'])}ì¼ / "
        f"ë…¸ì¶œ{_to_float1(min_cond['impressions'])} / í´ë¦­{_to_float1(min_cond['clicks'])})"
    )

def filter_min_conversion_condition(kw_total: pd.DataFrame, min_cond: Dict[str, float]) -> pd.DataFrame:
    return kw_total[
        (kw_total["orders_14d"] == 0)
        & (kw_total["active_days"] >= int(min_cond["active_days"]))
        & (kw_total["impressions"] >= min_cond["impressions"])
        & (kw_total["clicks"] >= min_cond["clicks"])
    ].copy()

def calc_base_threshold_t(df: pd.DataFrame) -> Dict[str, float]:
    # ê²€ìƒ‰ ì˜ì—­ ì¤‘ì—ì„œ ì£¼ë¬¸ì´ 1ê±´ì´ë¼ë„ ë°œìƒí•œ í‚¤ì›Œë“œë§Œ ë¶„ì„
    df_search = df[df["surface"] == SURF_SEARCH_VALUE].copy()
    df_search = df_search.groupby("keyword").filter(lambda g: g["orders_14d"].sum() > 0)
    if df_search.empty:
        st.warning("df_search ë¹„ì—ˆìŒ")
    else:
        st.info(f"ê²€ìƒ‰ ì˜ì—­ + ì „í™˜ 1 ì´ìƒ í‚¤ì›Œë“œ ìˆ˜: {df_search['keyword'].nunique()}")

    # í‰ê·  ì „í™˜ 1 ì´ìƒ ë§Œì¡±í•œ í‚¤ì›Œë“œ ìˆ˜ ì¶œë ¥
    valid_keywords = []
    for keyword, g in df_search.groupby("keyword"):
        g = g.sort_values("date").copy()
        g["cum_orders"] = g["orders_14d"].cumsum()
        g["day_number"] = range(1, len(g) + 1)
        g["avg_orders_per_day"] = g["cum_orders"] / g["day_number"]
        hit = g[g["avg_orders_per_day"] >= 1]
        if not hit.empty:
            valid_keywords.append(keyword)

    st.info(f"í‰ê·  ì „í™˜ 1 ì´ìƒ ë§Œì¡±í•œ í‚¤ì›Œë“œ ìˆ˜: {len(valid_keywords)}")

    if df_search.empty:
        return {"active_days": 0.0, "impressions": 0.0, "clicks": 0.0}

    rows = []
    for keyword, g in df_search.groupby("keyword"):
        g = g.sort_values("date").reset_index(drop=True)

        # âœ… 1. ì‹¤ì œ ë°œìƒí•œ ì „í™˜ ìˆ˜ ê³„ì‚°
        g["orders"] = g["orders_14d"].diff().fillna(g["orders_14d"])

        # âœ… 2. ëˆ„ì  ì „í™˜ / í‰ê·  ì „í™˜
        g["cum_orders"] = g["orders"].cumsum()
        g["day_number"] = range(1, len(g) + 1)
        g["avg_orders_per_day"] = g["cum_orders"] / g["day_number"]

        # âœ… 3. í‰ê·  ì „í™˜ì´ 1 ë„˜ëŠ” ìµœì´ˆ ì‹œì  ì°¾ê¸°
        hit = g[g["avg_orders_per_day"] >= 1]
        if hit.empty:
            continue

        pos = hit.index[0]
        g_until = g.iloc[:pos + 1]

        # âœ… 4. ìŠ¬ë¼ì´ìŠ¤ëœ ê°’ìœ¼ë¡œ ì§‘ê³„
        rows.append({
            "active_days": g_until["impressions"].gt(0).sum(),
            "impressions": g_until["impressions"].sum(),
            "clicks": g_until["clicks"].sum()
        })


    tdf = pd.DataFrame(rows)

    st.write("ì¡°ê±´ ë§Œì¡± í‚¤ì›Œë“œ ìˆ˜:", df_search["keyword"].nunique())
    st.write("rowsì— ë‹´ê¸´ ê°’ ìˆ˜:", len(rows))
    st.dataframe(tdf.describe())  # ðŸ‘ˆ ì¤‘ì•™ê°’ í™•ì¸

    tdf = pd.DataFrame(rows)
    if tdf.empty:
        return {"active_days": 0.0, "impressions": 0.0, "clicks": 0.0}

    return {
        "active_days": _median_1d(tdf["active_days"].astype(float)),
        "impressions": _median_1d(tdf["impressions"].astype(float)),
        "clicks": _median_1d(tdf["clicks"].astype(float)),
    }

    if not rows:
        st.warning("â— rows ë¹„ì—ˆìŒ â€” ì¡°ê±´ ë§Œì¡±í•´ë„ ìŠ¬ë¼ì´ìŠ¤ ì‹¤íŒ¨í–ˆì„ ê°€ëŠ¥ì„± ìžˆìŒ")
    else:
        st.info(f"rowsì— ë‹´ê¸´ í•­ëª© ìˆ˜: {len(rows)}")
        st.dataframe(pd.DataFrame(rows))  # âœ… ì‹¤ì œ ê°’ í™•ì¸    
    tdf = pd.DataFrame(rows)
    if tdf.empty:
        return {"active_days": 0.0, "impressions": 0.0, "clicks": 0.0}

    return {
        "active_days": _median_1d(tdf["active_days"].astype(float)),
        "impressions": _median_1d(tdf["impressions"].astype(float)),
        "clicks": _median_1d(tdf["clicks"].astype(float)),
    }

    tdf = pd.DataFrame(rows)

    if tdf.empty:
        st.error("âŒ rows ë¹„ì–´ ìžˆìŒ! ì¡°ê±´ ë§Œì¡±í•œ í‚¤ì›Œë“œê°€ ëˆ„ë½ëê±°ë‚˜ ìŠ¬ë¼ì´ìŠ¤ ì‹¤íŒ¨")
    else:
        st.success(f"âœ… rows ìˆ˜: {len(tdf)}")
        st.dataframe(tdf.describe())  # ì¤‘ì•™ê°’ í™•ì¸

# ====== Streamlit íƒ­ ë Œë”ë§ ======
def render_ad_analysis_tab(supabase):
    st.subheader("ê´‘ê³ ë¶„ì„ (ì´ 14ì¼ ê¸°ì¤€)")

    up = st.file_uploader("ë¡œìš°ë°ì´í„° ì—…ë¡œë“œ (xlsx/csv)", type=["xlsx", "csv"], key="ad_up")
    if up is None:
        st.info("íŒŒì¼ ì—…ë¡œë“œ í›„ ë¶„ì„ í¼ì´ ìƒì„±ë©ë‹ˆë‹¤.")
        return

    c1, c2, c3 = st.columns([2, 2, 2])
    with c1:
        product_name = st.text_input("ìƒí’ˆëª…", value="", key="ad_product")
    with c2:
        target_roas = st.number_input("ëª©í‘œ ROAS", min_value=0.0, value=0.0, step=10.0, key="ad_target")
    with c3:
        breakeven_roas = st.number_input("ì†ìµë¶„ê¸° ROAS", min_value=0.0, value=0.0, step=10.0, key="ad_be")

    note = st.text_input("ë©”ëª¨(ì„ íƒ)", value="", key="ad_note")

    if not product_name.strip():
        st.warning("ìƒí’ˆëª…ì„ ìž…ë ¥í•˜ì„¸ìš”.")
        return

    try:
        df_raw = pd.read_csv(up) if up.name.lower().endswith(".csv") else pd.read_excel(up)
    except Exception as e:
        st.error(f"íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return

    required = [DATE_COL, KW_COL, SURF_COL, IMP_COL, CLK_COL, COST_COL, ORD_COL, REV_COL]
    missing = [c for c in required if c not in df_raw.columns]
    if missing:
        st.error(f"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing}")
        return

    df = df_raw.copy()
    df["date"] = _to_date(df[DATE_COL])
    df = df[df["date"].notna()].copy()
    df["keyword"] = df[KW_COL].astype(str).fillna("").str.strip()
    df["surface"] = df[SURF_COL].astype(str).fillna("").str.strip()
    df["impressions"] = _to_int(df[IMP_COL])
    df["clicks"] = _to_int(df[CLK_COL])
    df["cost"] = _to_int(df[COST_COL])
    df["orders_14d"] = _to_int(df[ORD_COL])
    df["revenue_14d"] = _to_int(df[REV_COL])

    if df.empty:
        st.warning("ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    date_min, date_max = df["date"].min(), df["date"].max()

    st.markdown("### 1) ê¸°ë³¸ ì„±ê³¼ ì§€í‘œ")
    st.caption(f"ê¸°ê°„: {date_min} ~ {date_max}")

    total_cost = int(df["cost"].sum())
    total_rev = int(df["revenue_14d"].sum())
    total_orders = int(df["orders_14d"].sum())
    total_roas = round(_safe_div(total_rev, total_cost, 0) * 100, 2)

    df_imp_pos = df[df["impressions"] > 0]
    days = df_imp_pos.groupby("keyword")["date"].nunique().reset_index(name="active_days")

    kw = (
        df.groupby(["keyword", "surface"], as_index=False)[["impressions", "clicks", "cost", "orders_14d", "revenue_14d"]]
        .sum()
        .merge(days, on="keyword", how="left")
    )
    kw["active_days"] = kw["active_days"].fillna(0).astype(int)
    kw["ctr"] = (kw["clicks"] / kw["impressions"]).fillna(0).round(6)
    kw["cpc"] = (kw["cost"] / kw["clicks"]).replace([np.inf, -np.inf], 0).fillna(0).round(2)
    kw["roas_14d"] = (kw["revenue_14d"] / kw["cost"] * 100).replace([np.inf, -np.inf], 0).fillna(0).round(2)

    def _row(name, sub):
        c, r, o = int(sub["cost"].sum()), int(sub["revenue_14d"].sum()), int(sub["orders_14d"].sum())
        return {
            "ì˜ì—­": name,
            "ê´‘ê³ ë¹„": c,
            "ê´‘ê³ ë¹„ë¹„ìœ¨(%)": round(_safe_div(c, total_cost) * 100, 2),
            "ë§¤ì¶œ": r,
            "ë§¤ì¶œë¹„ìœ¨(%)": round(_safe_div(r, total_rev) * 100, 2),
            "ì£¼ë¬¸": o,
            "ì£¼ë¬¸ë¹„ìœ¨(%)": round(_safe_div(o, total_orders) * 100, 2),
            "ROAS": round(_safe_div(r, c) * 100, 2),
        }

    rows = [
        _row("ì „ì²´", df),
        _row("ê²€ìƒ‰", df[df["surface"] == SURF_SEARCH_VALUE]),
        _row("ë¹„ê²€ìƒ‰", df[df["surface"] != SURF_SEARCH_VALUE]),
    ]
    st.dataframe(pd.DataFrame(rows), use_container_width=True, hide_index=True)

    # 3) CPC-ëˆ„ì ë§¤ì¶œ ë¹„ì¤‘ & ì»·
    st.markdown("### 3) CPC-ëˆ„ì ë§¤ì¶œ ë¹„ì¤‘ & ì»·")
    conv = kw[kw["orders_14d"] > 0].sort_values("cpc")
    cpc_cut, cut_rev_share, aov_p50 = 0.0, 0.0, 0.0
    if conv.empty:
        st.warning("ì „í™˜ ë°œìƒ í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤. CPC_cutì„ 0ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
    else:
        total_conv_rev = conv["revenue_14d"].sum()
        conv["cum_rev"] = conv["revenue_14d"].cumsum()
        conv["cum_rev_share"] = (conv["cum_rev"] / total_conv_rev).clip(0, 1)
        x = conv["cpc"].to_numpy(dtype=float)
        y = conv["cum_rev_share"].to_numpy(dtype=float)
        if len(x) >= 3:
            x_n = (x - x.min()) / (x.max() - x.min() + 1e-12)
            y_n = (y - y.min()) / (y.max() - y.min() + 1e-12)
            idx = int(np.argmax(y_n - x_n))
            cpc_cut = float(x[idx])
        else:
            cpc_cut = float(x[-1])
        rev_above_cut = conv.loc[conv["cpc"] >= cpc_cut, "revenue_14d"].sum()
        cut_rev_share = round(_safe_div(rev_above_cut, total_conv_rev) * 100, 2)

        chart = alt.Chart(conv).mark_line().encode(x="cpc:Q", y="cum_rev_share:Q")
        # ê¸°ì¡´: íš¡ë³´ ì‹œìž‘ì„ 
        vline = alt.Chart(pd.DataFrame({"c": [cpc_cut]})).mark_rule(strokeDash=[6, 4]).encode(x="c:Q")

        # ì¶”ê°€: íš¡ë³´ ëë‚˜ëŠ” ì§€ì  ê³„ì‚°
        x = conv["cpc"].to_numpy(dtype=float)
        y = conv["cum_rev_share"].to_numpy(dtype=float)
        dy = np.gradient(y)
        end_idx = np.argmax(dy > 0.05) if (dy > 0.05).any() else -1

        if end_idx != -1:
            cpc_end = float(x[end_idx])
            vline2 = alt.Chart(pd.DataFrame({"c": [cpc_end]})).mark_rule(color="red", strokeDash=[2, 2]).encode(x="c:Q")
            chart += vline2
            st.caption(f"ðŸ“ˆ ëˆ„ì  ë§¤ì¶œ ìƒìŠ¹ ì§ì „ CPC: {round(cpc_end, 2)}ì›")

        # ê·¸ëž˜í”„ ì¶œë ¥
        st.altair_chart(chart + vline, use_container_width=True)
        st.caption(f"CPC_cut: {round(cpc_cut, 2)}ì› (ëˆ„ì ë§¤ì¶œ ë¹„ì¤‘ {cut_rev_share}%)")


        aov = (conv["revenue_14d"] / conv["orders_14d"]).dropna()
        aov_p50 = float(aov.quantile(0.5)) if not aov.empty else 0.0

    st.markdown("### 4) ì œì™¸ í‚¤ì›Œë“œ")

    ex_a = kw[(kw["orders_14d"] == 0) & (kw["cpc"] >= cpc_cut)].copy()
    ex_b = kw[(kw["active_days"] >= 7) & (kw["orders_14d"] > 0) & (kw["roas_14d"] < float(breakeven_roas))].copy()
    cpc_global_p50 = float(kw.loc[kw["clicks"] > 0, "cpc"].quantile(0.5)) if (kw["clicks"] > 0).any() else 0.0
    ex_c = kw[kw["orders_14d"] == 0].copy()
    ex_c["next_click_cost"] = np.where(ex_c["cpc"] > 0, ex_c["cpc"], cpc_global_p50)
    ex_c["cost_after_1click"] = ex_c["cost"] + ex_c["next_click_cost"]
    ex_c["roas_if_1_order"] = (aov_p50 / ex_c["cost_after_1click"] * 100).replace([np.inf, -np.inf], 0).fillna(0).round(2)
    ex_c = ex_c[ex_c["roas_if_1_order"] <= float(breakeven_roas)].copy()

    min_cond = build_min_conversion_condition(df)
    ex_d = filter_min_conversion_condition(kw, min_cond)

    def _show_df(title, dff, extra=None):
        st.markdown(f"#### {title} ({len(dff)}ê°œ)")
        cols = ["keyword", "surface", "active_days", "impressions", "clicks", "cost", "orders_14d", "revenue_14d", "ctr", "cpc", "roas_14d"]
        if extra:
            cols += extra
        st.dataframe(dff.sort_values("cost", ascending=False)[cols].head(200), use_container_width=True, hide_index=True)

    _show_df("a) CPC_cut ì´ìƒ ì „í™˜ 0", ex_a)
    _show_df("b) ìš´ì˜ ì¼ì£¼ì¼ ì´ìƒ ì†ìµë¶„ê¸° ë¯¸ë‹¬", ex_b)
    _show_df("c) ì „í™˜ ì‹œ ì˜ˆìƒ ROAS ë¯¸ë‹¬", ex_c, ["roas_if_1_order"])
    _show_df(f"d) {format_min_condition_label(min_cond)}", ex_d)

    st.markdown("### 5) Supabase ì €ìž¥")
    if st.button("âœ… ë¶„ì„ ê²°ê³¼ ì €ìž¥", key="ad_save"):
        try:
            run_id = str(uuid.uuid4())
            file_sha1 = hashlib.sha1(up.getvalue()).hexdigest()
            st.caption(f"íŒŒì¼ í•´ì‹œ: {file_sha1[:12]}â€¦")

            supabase.table("ad_analysis_runs").insert(
                {
                    "run_id": run_id,
                    "product_name": product_name.strip(),
                    "source_filename": up.name,
                    "source_rows": len(df_raw),
                    "date_min": str(date_min),
                    "date_max": str(date_max),
                    "note": note,
                    "target_roas": float(target_roas),
                    "breakeven_roas": float(breakeven_roas),
                    "cpc_cut": float(round(cpc_cut, 2)),
                }
            ).execute()

            rows_kw = kw.assign(run_id=run_id)[
                ["run_id", "keyword", "surface", "active_days", "impressions", "clicks", "cost", "orders_14d", "revenue_14d", "ctr", "cpc", "roas_14d"]
            ].to_dict(orient="records")
            for i in range(0, len(rows_kw), 1000):
                supabase.table("ad_analysis_keyword_total").upsert(rows_kw[i : i + 1000]).execute()

            artifacts = [
                {
                    "run_id": run_id,
                    "artifact_key": "settings",
                    "payload": {
                        "target_roas": target_roas,
                        "breakeven_roas": breakeven_roas,
                        "cpc_cut": float(round(cpc_cut, 2)),
                        "aov_p50": aov_p50,
                        "min_condition": min_cond,
                    },
                },
                {
                    "run_id": run_id,
                    "artifact_key": "exclusions",
                    "payload": {"a": ex_a["keyword"].tolist(), "b": ex_b["keyword"].tolist(), "c": ex_c["keyword"].tolist(), "d": ex_d["keyword"].tolist()},
                },
            ]
            supabase.table("ad_analysis_artifacts").upsert(artifacts).execute()
            st.success(f"ì €ìž¥ ì„±ê³µ (ID: {run_id})")

        except Exception as e:
            st.error(f"ì €ìž¥ ì‹¤íŒ¨: {e}")
